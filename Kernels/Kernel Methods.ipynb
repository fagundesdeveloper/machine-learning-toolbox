{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically a kernel is a way of computing the dot product of two vectors x and y in some (possibly high dimensional) feature space. This is wht kernel functions are sometimes called 'generalized dot product'. Given that kernels methods represent some dot product an intuititve way of thinking about kernel methods is as a similarity score. The objects being computed can be anything- given that the kernel method knows how to compute them. \n",
    "\n",
    "The simplest example is the linear kernel, also called the dot product. A more interesting example is the Gaussian Kernel, or the Radial Basis Kernel. Given two vectors the RBF will compute a similarity that diminishes with the radius of $ \\sigma $. In SVM the kernel methods are used inside the decision function to find the support vectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of SVMs is beautiful, the kernel trick is beautiful, and convex optimization is beautiful, and they stand quite independent.\n",
    " \n",
    "The kernel trick is that it can do complicated feature transformations without blowing up dimensionality. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
