{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees involve the greedy selection of the best split point from the dataset at each step. This algorithim makes decision trees susceptible to high variance if they are not pruned. One way around this is to  create multiple trees with different samples of the training dataset and combining predictions, this is called bootsrap aggregation or bagging. However if trees have similar split points then it mitigates the variance that was sought and predictions are likely to be similar. \n",
    "\n",
    " \n",
    "Using a decision tree to find the optimal decision is called solving the tree. To sovle a decision tree we work backwards. \n",
    "\n",
    "Descision trees are easy to read and use, this is very useful in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decision trees are fragile to changes in data value and lack reproducability- for that reason they are not as useful for prediction but because they are easy to interpret that can be a great tool for data exploration\n",
    "\n",
    "\n",
    "random forest are built to immediately address the concerns of reproducability and sensitivity to changes in data\n",
    "\n",
    "decision trees look for a split on every variable on every node while random forests look for a split on one variable- the variable with the largest association with target among candidate explanatory varianles, whih is a subset of the total list of explanatory variables. \n",
    "\n",
    "Each tree is grown by a subset of the explanatory variables at each node and a random subset of the sample for each tree in the forest. \n",
    "\n",
    "There is the bagged data, the subset chosen to generate the tree, and the ou-of-bag data- that is available to test the accuracy of the trees. \n",
    "\n",
    "The trees themselves are not interpretted, they are used collectively to rank the importance of variables in predicting the target of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split points are determimined by either the sum squared error for regression problemsor the gini cost funciton for claddification problems - \n",
    "\n",
    "splitting continues until the nodes contain the minimum number of training examples or a maximum depth is reached\n",
    "\n",
    "This is an exhaustive and greedy algorithm.\n",
    "\n",
    "make sure to plot split points and gini imputity or something \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "#add seaborn to make better the visualizations\n",
    "from IPython.display import display\n",
    "\n",
    "headers = ['index','refractive index', 'sodium', 'magnesium', 'aluminum', 'silicon', 'potassium', 'calcium', 'barium', 'iron', 'type of glass']\n",
    "df = pd.read_csv('glass_data.csv', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df.head(5))\n",
    "# display(df.describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
